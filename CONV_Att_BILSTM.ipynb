{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import attention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv_attLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, batch_size, dropout,use_bn, attn_head,\n",
    "                 attn_size, activation=\"ReLU\"):\n",
    "        super(Conv_attLSTM,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.attn_head = attn_head\n",
    "        self.attn_size = attn_size\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn\n",
    "        self.activation = getattr(nn, activation)()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.lstm = nn.LSTM(input_size = self.input_dim, hidden_size = self.hidden_dim, \n",
    "                            num_layers = self.num_layers, bidirectional = True)\n",
    "        self.attention = attention.Attention(self.batch_size, self.attn_head, self.attn_size, self.hidden_dim,\n",
    "                                             self.hidden_dim, self.dropout)\n",
    "        self.init_hidden_ = self.init_hidden()\n",
    "        self.convolusion = self.make_convolusion()\n",
    "        self.last_linear = nn.Linear(4*hidden_dim,1)\n",
    "        self.sigmoid =  nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h = torch.empty(2*self.num_layers, self.batch_size, self.hidden_dim).to(self.device)\n",
    "        c = torch.empty(2*self.num_layers, self.batch_size, self.hidden_dim).to(self.device)\n",
    "        return (nn.init.xavier_normal_(h),\n",
    "                nn.init.xavier_normal_(c))\n",
    "    \n",
    "    def make_convolusion(self): # 간단한 MLP를 만드는 함수\n",
    "        layers = []\n",
    "        layers.append(nn.Conv1d(11, 64, kernel_size=1))\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(64))  ##  nn.BatchNorm1d\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.MaxPool1d(5, stride=1,padding=2))\n",
    "        layers.append(nn.Dropout(self.dropout))    ##  nn.Dropout\n",
    "        layers.append(nn.Tanh())\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self.hidden 각각의 layer의 모든 hidden state 를 갖고있음\n",
    "        ## LSTM의 hidden state에는 tuple로 cell state포함, 0번째는 hidden state tensor, 1번째는 cell state\n",
    "        # input dimension은 (Batch, Time_step, Feature dimension) 순이다. (batch_first=True)\n",
    "        ## lstm_out : 각 time step에서의 lstm 모델의 output 값\n",
    "        ## lstm_out[-1] : 맨마지막의 아웃풋 값으로 그 다음을 예측\n",
    "        '''\n",
    "        x : [128, 10, 11]\n",
    "        conv_input : [128, 11, 10]\n",
    "        conv_output : [128, 64, 10]\n",
    "        lstm_input : [10, 128, 64]\n",
    "        lstm_output : [10, 128, 128]  # 마지막 항 concat(64,64) <- bidirectional\n",
    "        \n",
    "        attn_applied : [128, 128]\n",
    "        '''\n",
    "        \n",
    "        conv_output = self.convolusion(x.transpose(1,2).float())\n",
    "        lstm_input = conv_output.transpose(0,1).transpose(0,2).float().to(self.device)\n",
    "        lstm_out, self.hidden = self.lstm(lstm_input, self.init_hidden_)\n",
    "        \n",
    "\n",
    "        attn_applied, attn_weights = self.attention(lstm_out, lstm_out)\n",
    "\n",
    "        es = torch.cat([attn_applied, lstm_out[-1]], dim=1) # es : [128,256]\n",
    "        lin_es = self.last_linear(es).squeeze()\n",
    "        yhat = self.sigmoid(lin_es)\n",
    "        \n",
    "        return yhat, attn_weights, attn_applied\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py38_64",
   "language": "python",
   "name": "py38_64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
